{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zeroshot learning using BART and HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdGAqrrUKGgzd0cVtYVGm5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sljm12/machine_learning_notebooks/blob/master/Zeroshot_learning_using_BART_and_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZvJkfzmlzV"
      },
      "source": [
        "This is to try Zeroshot learning on News Articles Headings.\n",
        "\n",
        "The news headings are from IPTC Media Topics https://iptc.org/standards/media-topics/\n",
        "\n",
        "Baseline code from https://joeddav.github.io/blog/2020/05/29/ZSL.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXsKNqJYDCzV",
        "outputId": "d8dd2eb1-3e98-4314-c2ab-573de92dfd01"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 23 03:01:12 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    28W /  70W |   4205MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HopSz7r3frYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2bd291-1d06-4a7b-8c32-dfb800060464"
      },
      "source": [
        "!pip -qq install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3MB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiMsewtSl9j-"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60pKHbdriAa8",
        "outputId": "0560f9b4-de70-424b-89ca-82e7a6bfd97f"
      },
      "source": [
        "!wget https://www.iptc.org/std/NewsCodes/IPTC-MediaTopic-NewsCodes.xlsx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 02:34:04--  https://www.iptc.org/std/NewsCodes/IPTC-MediaTopic-NewsCodes.xlsx\n",
            "Resolving www.iptc.org (www.iptc.org)... 194.232.153.3\n",
            "Connecting to www.iptc.org (www.iptc.org)|194.232.153.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 628997 (614K) [application/vnd.openxmlformats-officedocument.spreadsheetml.sheet]\n",
            "Saving to: ‘IPTC-MediaTopic-NewsCodes.xlsx’\n",
            "\n",
            "IPTC-MediaTopic-New 100%[===================>] 614.25K   764KB/s    in 0.8s    \n",
            "\n",
            "2020-11-23 02:34:06 (764 KB/s) - ‘IPTC-MediaTopic-NewsCodes.xlsx’ saved [628997/628997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngsqR7e4iD8y"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "df=pd.read_excel(\"IPTC-MediaTopic-NewsCodes.xlsx\", skiprows=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "YKghalYwmD0w",
        "outputId": "6792d32f-ac7b-48a8-b060-6e416314bee7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NewsCode-URI</th>\n",
              "      <th>NewsCode-QCode (flat)</th>\n",
              "      <th>Level1/NewsCode</th>\n",
              "      <th>Level2/NewsCode</th>\n",
              "      <th>Level3/NewsCode</th>\n",
              "      <th>Level4/NewsCode</th>\n",
              "      <th>Level5/NewsCode</th>\n",
              "      <th>Level6/NewsCode</th>\n",
              "      <th>RetiredDate</th>\n",
              "      <th>Name (ar)</th>\n",
              "      <th>Definition (ar)</th>\n",
              "      <th>Name (de)</th>\n",
              "      <th>Definition (de)</th>\n",
              "      <th>Name (dk)</th>\n",
              "      <th>Definition (dk)</th>\n",
              "      <th>Name (en-GB)</th>\n",
              "      <th>Definition (en-GB)</th>\n",
              "      <th>Name (es)</th>\n",
              "      <th>Definition (es)</th>\n",
              "      <th>Name (fr)</th>\n",
              "      <th>Definition (fr)</th>\n",
              "      <th>Name (no)</th>\n",
              "      <th>Definition (no)</th>\n",
              "      <th>Name (pt-PT)</th>\n",
              "      <th>Definition (pt-PT)</th>\n",
              "      <th>Name (pt-BR)</th>\n",
              "      <th>Definition (pt-BR)</th>\n",
              "      <th>Name (se)</th>\n",
              "      <th>Definition (se)</th>\n",
              "      <th>Name (zh-Hans)</th>\n",
              "      <th>Definition (zh-Hans)</th>\n",
              "      <th>SubjectCode mapping</th>\n",
              "      <th>Wikidata mapping</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://cv.iptc.org/newscodes/mediatopic/01000000</td>\n",
              "      <td>medtop:01000000</td>\n",
              "      <td>medtop:01000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>فنون، ثقافة وترفيه</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kultur, Kunst, Unterhaltung und Medien</td>\n",
              "      <td>Alle Formen von Kunst, Unterhaltung, Kulturerb...</td>\n",
              "      <td>kunst, kultur, underholdning og medier</td>\n",
              "      <td>Kunst, kultur, underholdning og medier.</td>\n",
              "      <td>arts, culture, entertainment and media</td>\n",
              "      <td>All forms of arts, entertainment, cultural her...</td>\n",
              "      <td>Artes, cultura, entretenimiento y medios</td>\n",
              "      <td>Todas las formas de arte, entretenimiento cult...</td>\n",
              "      <td>Arts, culture, divertissement et médias</td>\n",
              "      <td>Toutes les formes d'arts, de divertissement, d...</td>\n",
              "      <td>kultur og underholdning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>artes, cultura, entretenimento e média</td>\n",
              "      <td>Todas as formas de arte, entretenimento, heran...</td>\n",
              "      <td>artes, cultura, entretenimento e mídia</td>\n",
              "      <td>Todas as formas de arte, entretenimento, heran...</td>\n",
              "      <td>Konst, kultur och nöje</td>\n",
              "      <td>Ämnen som rör utveckling och förfining av det ...</td>\n",
              "      <td>艺术、文化、娱乐和媒体</td>\n",
              "      <td>各种形式的艺术、娱乐、文化遗产和媒体</td>\n",
              "      <td>http://cv.iptc.org/newscodes/subjectcode/01000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://cv.iptc.org/newscodes/mediatopic/20000002</td>\n",
              "      <td>medtop:20000002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medtop:20000002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>فنون وتسلية</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kunst und Unterhaltung</td>\n",
              "      <td>Alle Formen von Kunst und Unterhaltung</td>\n",
              "      <td>kunst og underholdning</td>\n",
              "      <td>Alle former for kunst og underholdning.</td>\n",
              "      <td>arts and entertainment</td>\n",
              "      <td>All forms of arts and entertainment</td>\n",
              "      <td>Arte y entretenimiento</td>\n",
              "      <td>Todas las formas de arte y de entretenimiento</td>\n",
              "      <td>Arts et divertissement</td>\n",
              "      <td>Toutes les formes d'arts et de divertissement</td>\n",
              "      <td>kunst og underholdning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>arte e entretenimento</td>\n",
              "      <td>Todas as formas de arte e entretenimento.</td>\n",
              "      <td>arte e entretenimento</td>\n",
              "      <td>Todas as formas de arte e entretenimento.</td>\n",
              "      <td>Konst och underhållning</td>\n",
              "      <td>Alla former av konst och underhållning som int...</td>\n",
              "      <td>艺术与娱乐</td>\n",
              "      <td>各种形式的艺术和娱乐</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.wikidata.org/entity/Q2018526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://cv.iptc.org/newscodes/mediatopic/20000003</td>\n",
              "      <td>medtop:20000003</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medtop:20000003</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>تحريك</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Animation</td>\n",
              "      <td>Erzählung in Form von Computergrafik, als Lang...</td>\n",
              "      <td>animation</td>\n",
              "      <td>Historier fortalt gennem animerede tegninger e...</td>\n",
              "      <td>animation</td>\n",
              "      <td>Stories told through animated drawings in eith...</td>\n",
              "      <td>Animación</td>\n",
              "      <td>Historias contadas a través de dibujos animado...</td>\n",
              "      <td>Dessin animé</td>\n",
              "      <td>Histoires racontées à travers des dessins anim...</td>\n",
              "      <td>animasjonsfilm</td>\n",
              "      <td>NaN</td>\n",
              "      <td>animação</td>\n",
              "      <td>Técnica que permite dar a desenhos ou bonecos ...</td>\n",
              "      <td>animação</td>\n",
              "      <td>Animação, incluindo longas e curtas-metragens,...</td>\n",
              "      <td>Animation</td>\n",
              "      <td>Rörlig bild skapad av en serie teckningar, ler...</td>\n",
              "      <td>动画</td>\n",
              "      <td>长短篇的动画故事</td>\n",
              "      <td>http://cv.iptc.org/newscodes/subjectcode/01025000</td>\n",
              "      <td>https://www.wikidata.org/entity/Q11425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://cv.iptc.org/newscodes/mediatopic/20001135</td>\n",
              "      <td>medtop:20001135</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medtop:20001135</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kunstausstellung</td>\n",
              "      <td>Zeitlich begrenzte Präsentation von Kunst in e...</td>\n",
              "      <td>udstilling</td>\n",
              "      <td>Midlertidig præsentation af kunst eller særemn...</td>\n",
              "      <td>art exhibition</td>\n",
              "      <td>Temporary presentation of art in museums, art ...</td>\n",
              "      <td>Exposición de arte</td>\n",
              "      <td>Presentación temporal de arte en museos, salas...</td>\n",
              "      <td>Exposition artistique</td>\n",
              "      <td>Présentation temporaire d'art dans des musées,...</td>\n",
              "      <td>kunstutstilling</td>\n",
              "      <td>NaN</td>\n",
              "      <td>exposição artística</td>\n",
              "      <td>Apresentação temporária de arte em museus, sal...</td>\n",
              "      <td>exposição artística</td>\n",
              "      <td>Apresentação temporária de arte em museus, sal...</td>\n",
              "      <td>Konstutställning</td>\n",
              "      <td>Ett tillfälle då en eller flera konstverk visa...</td>\n",
              "      <td>艺术展览</td>\n",
              "      <td>在博物馆、艺术大厅或画廊举办的短期艺术品展出</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.wikidata.org/entity/Q667276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://cv.iptc.org/newscodes/mediatopic/20000004</td>\n",
              "      <td>medtop:20000004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>medtop:20000004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>كرتون</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Comic</td>\n",
              "      <td>Zeichnungen, wie Karikaturen und Comics, die o...</td>\n",
              "      <td>tegneserie</td>\n",
              "      <td>Tegninger, både som underholding og redaktione...</td>\n",
              "      <td>cartoon</td>\n",
              "      <td>Drawings, such as editorial cartoons and comic...</td>\n",
              "      <td>Caricaturas</td>\n",
              "      <td>Dibujos, como caricaturas políticas o tiras có...</td>\n",
              "      <td>Bande dessinée</td>\n",
              "      <td>Dessins (dessins de presse, caricatures, bande...</td>\n",
              "      <td>tegneserie</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cartoon</td>\n",
              "      <td>Banda desenhada, geralmente de caráter humorís...</td>\n",
              "      <td>cartoon</td>\n",
              "      <td>Imagens como caricaturas editoriais e tiras de...</td>\n",
              "      <td>Serieteckningar</td>\n",
              "      <td>Bilder framställda på andra sätt än med kamera...</td>\n",
              "      <td>卡通</td>\n",
              "      <td>漫画，经常使用幽默或讽刺的绘画形式，如社论漫画和连环漫画</td>\n",
              "      <td>http://cv.iptc.org/newscodes/subjectcode/01024000</td>\n",
              "      <td>https://www.wikidata.org/entity/Q627603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       NewsCode-URI  ...                          Wikidata mapping\n",
              "0  http://cv.iptc.org/newscodes/mediatopic/01000000  ...                                       NaN\n",
              "1  http://cv.iptc.org/newscodes/mediatopic/20000002  ...  https://www.wikidata.org/entity/Q2018526\n",
              "2  http://cv.iptc.org/newscodes/mediatopic/20000003  ...    https://www.wikidata.org/entity/Q11425\n",
              "3  http://cv.iptc.org/newscodes/mediatopic/20001135  ...   https://www.wikidata.org/entity/Q667276\n",
              "4  http://cv.iptc.org/newscodes/mediatopic/20000004  ...   https://www.wikidata.org/entity/Q627603\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNYTCxQziKDn"
      },
      "source": [
        "# Get the 2nd level headings because they are more detailed that the first level headings\n",
        "df_top_level = df[df[\"Level2/NewsCode\"].notnull()]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjxCA_wqmObc",
        "outputId": "d927a805-3557-4312-fbd5-7f6f1658dfb2"
      },
      "source": [
        "df_top_level.size"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XqTxcXQjVjW"
      },
      "source": [
        "language = \"zh-Hans\"\n",
        "#language = \"en-GB\"\n",
        "topics = [i for i in df_top_level[\"Name (%s)\"% (language)]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SyPRaLd0dZJ",
        "outputId": "886e2d79-2f36-41df-9191-fa0b3655fcc6"
      },
      "source": [
        "topics[0:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['艺术与娱乐', '文化', '大众传媒', '恐怖活动', '武装冲突']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1UlQTujmfah"
      },
      "source": [
        "# Zeroshot learning portion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoNfBM3p2m5B"
      },
      "source": [
        "Use facebook/bart-large-mnli for english. https://huggingface.co/facebook/bart-large-mnli\n",
        "\n",
        "Use xlm-roberta-large-xnli for multilingual analysis.\n",
        "For the list of languages look https://huggingface.co/joeddav/xlm-roberta-large-xnli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNtsRM_FwvyD"
      },
      "source": [
        "model_name = 'facebook/bart-large-mnli' #@param ['facebook/bart-large-mnli', \"joeddav/xlm-roberta-large-xnli\"] {allow-input: true}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07PQjoIxe4MM",
        "outputId": "02395102-cdfa-4a50-ed3d-de9bc11fde45"
      },
      "source": [
        "from transformers import BartForSequenceClassification, BartTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
        "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
        "#model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# pose sequence as a NLI premise and label (politics) as a hypothesis\n",
        "premise = 'Who are you voting for in 2020?'\n",
        "hypothesis = 'This text is about politics.'\n",
        "\n",
        "if torch.cuda.current_device >=0:\n",
        "    model.to('cuda')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForSequenceClassification(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): EncoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): DecoderLayer(\n",
              "          (self_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (classification_head): BartClassificationHead(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zugs5anRoy1w"
      },
      "source": [
        "\n",
        "def predict(tokenizer, model, premise, hypothesis_sentence, category):\n",
        "  hypothesis = hypothesis_sentence % (category)\n",
        "\n",
        "  input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
        "  if torch.cuda.current_device() >=0:\n",
        "    input_ids=input_ids.cuda()\n",
        "\n",
        "  logits = model(input_ids)[0]\n",
        "\n",
        "  #print(logits)\n",
        "  entail_contradiction_logits = logits[:,[0,2]]\n",
        "  #print(entail_contradiction_logits)\n",
        "  probs = entail_contradiction_logits.softmax(dim=1)\n",
        "  true_prob = probs[:,1].item() * 100\n",
        "  #print(f'Probability that the label is true: {true_prob:0.2f}%')\n",
        "  return true_prob"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Y0CQ3K797J"
      },
      "source": [
        "hypothesis_sentence = {\n",
        "    \"en-GB\": \"This article is about %s\",\n",
        "    \"zh-Hans\": \"这篇文章是关于%s\"\n",
        "}"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_BM6MKB9Qop"
      },
      "source": [
        "sentence = hypothesis_sentence[language]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h61hwYG8HM5u"
      },
      "source": [
        "Just some notes on the using GPU, without GPU it took 2mins with GPU it took 4s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d20jXtCpI3P",
        "outputId": "0f0f0888-dff5-425e-8d37-f27f747a8708"
      },
      "source": [
        "import datetime\n",
        "start_time=datetime.datetime.now()\n",
        "premise = \"《研究報告》電動自駕飆速 AI基金樂歪\"\n",
        "#hypothesis_sentence = \"This article is about %s\"\n",
        "\n",
        "results = []\n",
        "for i in topics:\n",
        "  prob = predict(tokenizer, model, premise, sentence, i)\n",
        "  results.append({\n",
        "      \"category\":i,\n",
        "      \"prob\":prob\n",
        "  })\n",
        "end_time=datetime.datetime.now()\n",
        "print(end_time-start_time)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:00:04.224723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUMRWJ63x5fc"
      },
      "source": [
        "result_frame = pd.DataFrame(data=results)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "6b7j55Mix_hV",
        "outputId": "0a203387-59f8-4283-e058-79481ca5b7b1"
      },
      "source": [
        "result_frame.sort_values(\"prob\",ascending=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>科学研究</td>\n",
              "      <td>99.933004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>科研机构</td>\n",
              "      <td>99.728715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>自然科学</td>\n",
              "      <td>99.623114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>生物医学科学</td>\n",
              "      <td>99.014181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>技术和工程</td>\n",
              "      <td>98.902285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>法律</td>\n",
              "      <td>0.016649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>政府</td>\n",
              "      <td>0.015955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>人类</td>\n",
              "      <td>0.014828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>政府政策</td>\n",
              "      <td>0.014573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>学校</td>\n",
              "      <td>0.014274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>109 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   category       prob\n",
              "81     科学研究  99.933004\n",
              "80     科研机构  99.728715\n",
              "79     自然科学  99.623114\n",
              "77   生物医学科学  99.014181\n",
              "84    技术和工程  98.902285\n",
              "..      ...        ...\n",
              "14       法律   0.016649\n",
              "62       政府   0.015955\n",
              "91       人类   0.014828\n",
              "63     政府政策   0.014573\n",
              "27       学校   0.014274\n",
              "\n",
              "[109 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q-QVbscx6Bm"
      },
      "source": [
        "# Original Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGvgzzCLgECT",
        "outputId": "d4eb930a-2852-45cb-ca0e-495212579bbc"
      },
      "source": [
        "# run through model pre-trained on MNLI\n",
        "input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
        "print(input_ids)\n",
        "logits = model(input_ids)[0]\n",
        "\n",
        "print(logits)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    0, 12375,    32,    47,  3434,    13,    11,  2760,   116,     2,\n",
            "             2,   713,  2788,    16,    59,  2302,     4,     2]])\n",
            "tensor([[-2.5443,  0.8770,  1.3904]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXDvzRw5gO4V",
        "outputId": "c9e7e15b-87fc-49fb-dc43-a92885ce30d3"
      },
      "source": [
        "# we throw away \"neutral\" (dim 1) and take the probability of\n",
        "# \"entailment\" (2) as the probability of the label being true \n",
        "entail_contradiction_logits = logits[:,[0,2]]\n",
        "print(entail_contradiction_logits)\n",
        "probs = entail_contradiction_logits.softmax(dim=1)\n",
        "true_prob = probs[:,1].item() * 100\n",
        "print(f'Probability that the label is true: {true_prob:0.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.5443,  1.3904]], grad_fn=<IndexBackward>)\n",
            "Probability that the label is true: 98.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFcEsAOQhE6T",
        "outputId": "ecd210f2-c962-4fb3-d82e-1696857b3bcd"
      },
      "source": [
        "logits[:,[0,1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.5443,  0.8770]], grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJryFjZjhPnz",
        "outputId": "59d2af14-b83d-4751-8e59-f0967d26c572"
      },
      "source": [
        "probs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0192, 0.9808]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FasQl_dKhe4U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}